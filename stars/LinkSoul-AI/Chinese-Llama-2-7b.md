---
project: Chinese-Llama-2-7b
stars: 2228
description: å¼€æºç¤¾åŒºç¬¬ä¸€ä¸ªèƒ½ä¸‹è½½ã€èƒ½è¿è¡Œçš„ä¸­æ–‡ LLaMA2 æ¨¡å‹ï¼
url: https://github.com/LinkSoul-AI/Chinese-Llama-2-7b
---

Chinese Llama 2 7B
==================

å…¨éƒ¨å¼€æºï¼Œå®Œå…¨å¯å•†ç”¨çš„**ä¸­æ–‡ç‰ˆ Llama2 æ¨¡å‹åŠä¸­è‹±æ–‡ SFT æ•°æ®é›†**ï¼Œè¾“å…¥æ ¼å¼ä¸¥æ ¼éµå¾ª _llama-2-chat_ æ ¼å¼ï¼Œå…¼å®¹é€‚é…æ‰€æœ‰é’ˆå¯¹åŸç‰ˆ _llama-2-chat_ æ¨¡å‹çš„ä¼˜åŒ–ã€‚

åŸºç¡€æ¼”ç¤º
----

åœ¨çº¿è¯•ç©
----

> Talk is cheap, Show you the Demo.

-   Demo åœ°å€ / HuggingFace Spaces
-   Colab (FP16/éœ€è¦å¼€å¯é«˜RAM,å…è´¹ç‰ˆæ— æ³•ä½¿ç”¨)
-   Colab (INT4/éœ€è¦å¼€å¯é«˜RAM,å…è´¹ç‰ˆæ— æ³•ä½¿ç”¨)

æœ€æ–°æ›´æ–°
----

-   10æœˆ26æ—¥ æä¾›å§‹æ™ºAIé“¾æ¥Chinese Llama2 Chat Model ğŸ”¥ğŸ”¥ğŸ”¥
-   8æœˆ24æ—¥ æ–°åŠ ModelScopeé“¾æ¥Chinese Llama2 Chat Model ğŸ”¥ğŸ”¥ğŸ”¥
-   7æœˆ31å· åŸºäº Chinese-llama2-7b çš„ä¸­è‹±åŒè¯­è¯­éŸ³-æ–‡æœ¬ LLaSM å¤šæ¨¡æ€æ¨¡å‹å¼€æº ğŸ”¥ğŸ”¥ğŸ”¥
-   7æœˆ31å· åŸºäº Chinese-llama2-7b çš„ä¸­è‹±åŒè¯­è§†è§‰-æ–‡æœ¬ Chinese-LLaVA å¤šæ¨¡æ€æ¨¡å‹å¼€æº ğŸ”¥ğŸ”¥ğŸ”¥
-   7æœˆ26å· Chinese-llama2-7b-ggml æ¨¡å‹å¼€æºğŸ”¥ğŸ”¥
-   7æœˆ23æ—¥ æ›´æ–°7bæ¨¡å‹ï¼Œæ·»åŠ APIï¼Œæä¾›4bité‡åŒ–æ¨¡å‹ğŸ”¥ğŸ”¥
-   7æœˆ22å· SFTè®­ç»ƒ/æ¨ç†ä»£ç ä¸Šçº¿ ğŸ”¥
-   7æœˆ21å· docker ä¸€é”®éƒ¨ç½²ä¸Šçº¿ ğŸ”¥
-   7æœˆ21å· demoä¸Šçº¿ ğŸ”¥
-   7æœˆ21å· ä¸­è‹±åŒè¯­ SFT æ•°æ®å¼€æº ğŸ”¥ğŸ”¥
-   7æœˆ21å· Chinese-llama2-7b æ¨¡å‹å¼€æº ğŸ”¥ğŸ”¥

èµ„æºä¸‹è½½
----

-   æ¨¡å‹ä¸‹è½½
    
    -   å§‹æ™ºAI: Chinese Llama2 Chat Model
    -   ModelScope: Chinese Llama2 Chat Model
    -   HuggingFace: Chinese Llama2 Chat Model
    -   ç™¾åº¦ç½‘ç›˜: 1.0 æ­£å¼ç‰ˆ
    -   ç™¾åº¦ç½‘ç›˜: 1.1 åŠ å¼ºç«åŠ›ç‰ˆ
-   4bité‡åŒ–
    
    -   HuggingFaceï¼šChinese Llama2 4bit Chat Model
    -   ç™¾åº¦ç½‘ç›˜: Chinese Llama2 4bit Chat Model
-   GGML Q4 æ¨¡å‹ï¼š
    
    -   https://huggingface.co/LinkSoul/Chinese-Llama-2-7b-ggml
    -   https://huggingface.co/rffx0/Chinese-Llama-2-7b-ggml-model-q4\_0
    -   https://huggingface.co/soulteary/Chinese-Llama-2-7b-ggml-q4
    -   ç™¾åº¦ç½‘ç›˜: Chinese-Llama-2-7b-ggml

> æˆ‘ä»¬ä½¿ç”¨äº†ä¸­è‹±æ–‡ SFT æ•°æ®é›†ï¼Œæ•°æ®é‡ 1000 ä¸‡ã€‚

-   æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/LinkSoul/instruction\_merge\_set

å¿«é€Ÿæµ‹è¯•
----

from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

model\_path \= "LinkSoul/Chinese-Llama-2-7b"

tokenizer \= AutoTokenizer.from\_pretrained(model\_path, use\_fast\=False)
model \= AutoModelForCausalLM.from\_pretrained(model\_path).half().cuda()
streamer \= TextStreamer(tokenizer, skip\_prompt\=True, skip\_special\_tokens\=True)

instruction \= """\[INST\] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} \[/INST\]"""

prompt \= instruction.format("ç”¨ä¸­æ–‡å›ç­”ï¼ŒWhen is the best time to visit Beijing, and do you have any suggestions for me?")
generate\_ids \= model.generate(tokenizer(prompt, return\_tensors\='pt').input\_ids.cuda(), max\_new\_tokens\=4096, streamer\=streamer)

Docker
------

ä½ å¯ä»¥ä½¿ç”¨ä»“åº“ä¸­çš„ Dockerfileï¼Œæ¥å¿«é€Ÿåˆ¶ä½œåŸºäº Nvidia æœ€æ–°ç‰ˆæœ¬çš„ `nvcr.io/nvidia/pytorch:23.06-py3` åŸºç¡€é•œåƒï¼Œåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨å®¹å™¨æ¥è¿è¡Œä¸­æ–‡çš„ LLaMA2 æ¨¡å‹åº”ç”¨ã€‚

docker build -t linksoul/chinese-llama2-chat .

é•œåƒæ„å»ºå®Œæ¯•ï¼Œä½¿ç”¨å‘½ä»¤è¿è¡Œé•œåƒå³å¯ï¼š

docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it -v \`pwd\`/LinkSoul:/app/LinkSoul -p 7860:7860 linksoul/chinese-llama2-chat

GGML / Llama.cpp
----------------

æƒ³è¦åœ¨ CPU ç¯å¢ƒè¿è¡Œ LLaMA2 æ¨¡å‹ä¹ˆï¼Ÿä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•å§ã€‚

-   ä½¿ç”¨ `ggml/convert_to_ggml.py` è¿›è¡Œè½¬æ¢æ“ä½œï¼Œè¯¦è§è„šæœ¬æ”¯æŒçš„ CLI å‚æ•°ã€‚
-   æˆ–ä½¿ç”¨ `docker pull soulteary/llama2:converter` ä¸‹è½½æ¨¡å‹æ ¼å¼è½¬æ¢å·¥å…·é•œåƒï¼Œåœ¨ Docker å®¹å™¨ä¸­ä½¿ç”¨ä¸‹é¢çš„ä¸¤æ¡å‘½ä»¤å®Œæˆæ“ä½œï¼ˆæ•™ç¨‹ æ„å»ºèƒ½å¤Ÿä½¿ç”¨ CPU è¿è¡Œçš„ MetaAI LLaMA2 ä¸­æ–‡å¤§æ¨¡å‹ ï¼‰ï¼š

python3 convert.py /app/LinkSoul/Chinese-Llama-2-7b/ --outfile /app/LinkSoul/Chinese-Llama-2-7b-ggml.bin
./quantize /app/LinkSoul/Chinese-Llama-2-7b-ggml.bin /app/LinkSoul/Chinese-Llama-2-7b-ggml-q4.bin q4\_0

é‡åŒ–é…ç½®çš„å®šä¹‰:

è½¬è‡ª: https://www.reddit.com/r/LocalLLaMA/comments/139yt87/notable\_differences\_between\_q4\_2\_and\_q5\_1/

-   q4\_0 = 32 numbers in chunk, 4 bits per weight, 1 scale value at 32-bit float (5 bits per value in average), each weight is given by the common scale \* quantized value.
    
-   q4\_1 = 32 numbers in chunk, 4 bits per weight, 1 scale value and 1 bias value at 32-bit float (6 bits per value in average), each weight is given by the common scale \* quantized value + common bias.
    
-   q4\_2 = same as q4\_0, but 16 numbers in chunk, 4 bits per weight, 1 scale value that is 16-bit float, same size as q4\_0 but better because chunks are smaller.
    
-   q4\_3 = already dead, but analogous: q4\_1 but 16 numbers in chunk, 4 bits per weight, scale value that is 16 bit and bias also 16 bits, same size as q4\_1 but better because chunks are smaller.
    
-   q5\_0 = 32 numbers in chunk, 5 bits per weight, 1 scale value at 16-bit float, size is 5.5 bits per weight
    
-   q5\_1 = 32 numbers in a chunk, 5 bits per weight, 1 scale value at 16 bit float and 1 bias value at 16 bit, size is 6 bits per weight.
    
-   q8\_0 = same as q4\_0, except 8 bits per weight, 1 scale value at 32 bits, making total of 9 bits per weight.
    

APIéƒ¨ç½²
-----

é¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ api.pyï¼š

python api.py

é»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨

curl -X POST "http://127.0.0.1:8000" \\
     -H 'Content-Type: application/json' \\
     -d '{"prompt": "ä½ å¥½", "history": \[\]}'

å¾—åˆ°çš„è¿”å›å€¼ä¸º

{
  "response":" ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜å’Œè¿›è¡Œå¯¹è¯ã€‚è¯·é—®ä½ æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ ",
  "history":\[\["<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nä½ å¥½"," ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜å’Œè¿›è¡Œå¯¹è¯ã€‚è¯·é—®ä½ æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ "\]\],
  "status":200,
  "time":"2023-08-01 09:22:16"
}

å¦‚ä½•è®­ç»ƒ
----

DATASET="LinkSoul/instruction\_merge\_set"

DATA\_CACHE\_PATH="hf\_datasets\_cache"
MODEL\_PATH="/PATH/TO/TRANSFORMERS/VERSION/LLAMA2"

output\_dir="./checkpoints\_llama2"

torchrun --nnodes=1 --node\_rank=0 --nproc\_per\_node=8 \\
    --master\_port=25003 \\
        train.py \\
        --model\_name\_or\_path ${MODEL\_PATH} \\
        --data\_path ${DATASET} \\
        --data\_cache\_path ${DATA\_CACHE\_PATH} \\
        --bf16 True \\
        --output\_dir ${output\_dir} \\
        --num\_train\_epochs 1 \\
        --per\_device\_train\_batch\_size 4 \\
        --per\_device\_eval\_batch\_size 4 \\
        --gradient\_accumulation\_steps 1 \\
        --evaluation\_strategy 'no' \\
        --save\_strategy 'steps' \\
        --save\_steps 1200 \\
        --save\_total\_limit 5 \\
        --learning\_rate 2e-5 \\
        --weight\_decay 0. \\
        --warmup\_ratio 0.03 \\
        --lr\_scheduler\_type cosine \\
        --logging\_steps 1 \\
        --fsdp 'full\_shard auto\_wrap' \\
        --fsdp\_transformer\_layer\_cls\_to\_wrap 'LlamaDecoderLayer' \\
        --tf32 True \\
        --model\_max\_length 4096 \\
        --gradient\_checkpointing True

ç›¸å…³é¡¹ç›®
----

-   Llama2
-   soulteary/docker-llama2-chat

é¡¹ç›®åè®®
----

Apache-2.0 license

å¾®ä¿¡äº¤æµç¾¤
-----
